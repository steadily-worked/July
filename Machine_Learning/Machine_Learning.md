7/22 
# <1> 머신 러닝 기본기

## (1) 머신 러닝이란?


1. 머신러닝이란?

머신러닝 프로그램은 프로그램을 수행할 수록 점점 더 잘하게 됨. 경험을 통해서 스스로 업무 
능력을 키움.

ex) 스팸 메일 분류 프로그램

작업 : 스팸 분류,

경험 : 새로운 이메일을 보고 분류

스팸 분류를 얼마나 잘하는지가 이 프로그램의 성능을 나타냄.

프로그램이 더 많은 이메일을 봐서 스팸을 알아보는 실력이 향상된다면 머신러닝 프로그램이라고 부를 수 있는 것.


머신러닝을 할 때는 컴퓨터가 규칙을 직접 찾아내도록 하게 할 수 있다. (데이터를 이용해서)

새로 주어진 데이터를 통해 정교한 데이터셋을 만들 수 있음



2. 머신러닝이 핫해진 이유?

(1) 세상에 사용가능한 데이터가 많아졌기 때문

개인 컴퓨터가 생기고 스마트폰 등이 보급되면서 데이터가 빠르게 쌓이기 시작.

데이터가 엄청나게 많이 쌓이면서 최근에 머신러닝을 사용할 수 있게 됨.

(2) 컴퓨터 성능이 좋아졌기 때문

컴퓨터가 안좋던 시절엔 프로그램 학습에 시간이 너무 오래 걸렸었다. 그래서 많은 머신러닝 
기법이 잠들어 있었음.

(3) 머신러닝의 활용성이 증명되었기 때문

ex) 유튜브. 시청 데이터가 많이 쌓이면서 이 데이터를 기반으로 머신 러닝을 함으로써 
좋아하는 컨텐츠를 추천해주고 맞춤 영상을 제공해 줌.

현재 머신러닝 전문가가 매우 부족한 상황..


3. 인공지능? 빅데이터? 머신러닝?

빅데이터

엄청나게 많은 양의 데이터를 다루는 분야

엄청 많은 양의 데이터의 효율적인 보관/처리법, 분석 방법 등

인공지능

컴퓨터 프로그램이 인간처럼 생각/행동하게 하는 학문

딥러닝

머신러닝 기법 중 하나.

층이 “깊어"진다 -> “딥"러닝

인공지능 > 머신러닝 > 딥러닝 (인공지능으로 갈수록 큰 분야)


4. 학습의 유형

1. 지도 학습 (supervised learning)

- "답이 있고 이 답을 맞추는 게 학습의 목적. 비지도 학습에는 답이 없음.

ex) 스팸 메일 분류 프로그램 -> 우리는 메일을 보고 스팸인지 아닌지를 맞추고 싶은 것이므로 답이 있는 문제를 해결하는 것이다. 즉 지도학습에 해당하는 것.

ex2) 아파트 데이터 -> 위치, 크기, 건설 연도, 교통 정보 등의 데이터를 통해 또 다른 아파트의 가격을 예측하고 싶어 하는 경우. "아파트의 가격"이 바로 답이므로 즉 이도 지도학습에 해당한다고 볼 수 있다.

지도 학습도 크게 두 개로 나뉜다.

1. 분류 (Classification)

- ex) 스팸인지 아닌지 두 옵션 중 하나를 고르는 것.

2. 회귀 (Regression)

- ex) 아파트 가격을 예측하는 것. 몇가지 옵션 중에 고르는 게 아니라 무수히 많고 연속적인 값들 중에 맞추는 것이므로 이것은 회귀 문제이다.


2. 비지도 학습 (Unsupervised learning)

- '답'이 없고 이 답을 맞추는 게 학습의 목적

- 프로그램이 알아서 학습하게 하는 것 .. (프로그램이 자기 나름의 기준을 세워서 그것에 맞게 분류하게끔 알아서 시키는 것)

지도 학습 : 우선 제작, 직관적, 더 많이 사용됨.

5. k-NN 알고리즘 (k-Nearest Neighbors Algorithm)

- 많은 경험 -> 성능 향상 -> 머신 러닝이라고 할 수 있음.

6. 머신 러닝의 수학

- 머신 러닝 -> 컴퓨터 과학 + 수학 (선형대수 ,미분, 통계, 확률)

7. 머신 러닝 공부법

(1) 영상을 잠깐 멈추거나 이해가 안 되는 부분들을 반복적으로 들으면서 생각해본다.

(2) 해당 영상에 대한 정리 노트가 있으면 읽으면서 생각해본다.

(3) 아무리 생각해도 이해가 안되는 부분이 있으면 질문을 남긴다.


## (2) 선형대수학 필요한 만큼만 배우기

1. 일차식과 일차 함수

- 선형 대수학 : 일차식이나 일차 함수를 공부하는 학문

2. 일차 함수 표기법

- f(x, y) = 3x+6y+4

- f(x0, x1, x2, ..., xn) = a0x0 + a1x1 + ... + anxn + b(상수항)

어려워 보일 수 있지만 일차식이랑 똑같은 것임.

3. 행렬과 벡터

- 행렬 : Matrix

A =

[1 1 0 2]

[2 1 4 1]

[0 3 2 1]

1, 2, 0 -> 행, 0, 3, 2, 1 -> 열 3 * 4 행렬!

벡터 : 행이나 열이 하나밖에 없는 행렬! 5*1 행렬 or 1*5 행렬.

- 보통 벡터는 열 벡터임. ( 5 * 1 류의 세로줄 행렬 ) -> 5차원 행렬이라고 부른다!



11. 요소별 곱하기

- A = 2 * 2 행렬 [1, 2, 3, 4], B = 2 * 2 행렬 [-1, 2, 3, 1] 일 때
A∘B -> 2 * 2 행렬 [-1, 4, 9, 4] 임.

13. numpy로 행렬 연산하기

- A @ B (= np.dot(A, B)) -> 내적곱, A * B -> 요소별 곱하기

15. 전치 행렬, 렬
1) 진치 행렬 A = 3 * 2 행렬 [3, 2, 2, 1, 3, 5] 일때 At = 3 * 2 행렬 [3, 2, 3, 2, 1, 5]

2) 역행렬 -> A-1은 .. 1/ad-bc[d -b -c a] 이다.

16. numpy에서 전치, 단위, 역행렬 사용하기

A_tranpose = A.T 하고 A_tranpose 하면 전치행렬화 됨.
I = np.identity(3) 하고 I 하면 3 * 3 단위행렬이 나옴.

역행렬 -> np.linalg.pinv(A)를 하면 됨. 역행렬이 없는 경우에는, 역행렬과 최대한 비슷한 효과를 주는 행렬을 적용함.

A_inverse = np.linalg.pinv(A) 하고 A_inverse 하면 역행렬화 됨.
A @ A_inverse 를 해줘서 단위행렬이 나오면 되는 것. 11, 22, 33부분은 0으로 나오고, 나머지 부분은 6.66133815e-16와 같이 0에 거의 가까운 소수형으로 나옴.. 


19. 선형대수학과 행렬 / 벡터

- 아무리 복잡한 선형 시스템도 행렬과 벡터로 쉽게 표현할 수 있음!!

[2 -4 1] [x0] = [3]<br>
[3 1 -6] [x1] = [10]<br>
[1 1 1 ] [x2] = [5]

를 한 번에 표현하면 ..

[2x0 - 4x1 + x2] = [3]<br>
[3x0 + x1 - 6x2] = [10]<br>
[x0 + x1 + x2] = [5] 이다!

20. 선형대수학이 머신 러닝에 필요한 이유

- 아파트 가격 예측 -> 집값은, 크기(a1), 지하철역 거리(a2), 층수(a3) 에 따라 정해진다고 하자.

- 첫번째 아파트 가격 : 110*a1 + 400*a2 + 20*a3 .... 이렇게 n개를 만들었다고 해 보자. 이 식들은 모두 일차식이다. 

X를 ..<br>
[110 400 20]<br>
[100 1000 5]<br>
[180 10 30]<br>
[50 300 5]<br>
[  ...  ] 이처럼 각 요소들의 가격이 주어져 있을 때

a = 3 * 1 행렬 [a1, a2, a3]이라고 했을 때

모든 집 값은 Xa로 나타낼 수 있다. 이렇게 깔끔하게 요약하여 나타낼 수 있다는 점에서 선형대수학이 필요한 것.

## (3) 미분 필요한 만큼만 배우기

1. 함수

- input 값을 받고 함수를 일련의 과정을 통해서 output 을 출력한다.

- f(x,y) = x + 2y. input이 여러개일 수도 있다.

3. 평균 변화율

기울기 : 얼마나 빠르게 늘어나고, 얼마나 빠르게 줄어드는지를 나타냄

평균 변화율 : f(b) - f(a) / b - a, f(a + h) - f(a) / h

7. 가장 가파른 방향(2차원)

- 최대한 가파르게 그래프를 올라가고 싶다면 .. 해당 지점의 기울기가 -일 경우 왼쪽으로 가야하고 +일 경우 오른쪽으로 가야한다. ( ex) x^2-2x+1 )

- 기울기는 그래프가 해당 지점에서 얼마나 기울어져 있는지도 알려 주지만, 어떤 방향으로 가야 가장 가파르게 올라갈 수 있는지에 대한, 그런 정보도 제공해 준다. 이걸 이해하는 게 머신 러닝에서 핵심적이므로 잘 기억해 둬야 한다.

10. 극소점, 극대점 노트

- 극소점 및 극대점은 순간 변화율이 0인 지점이다.

- 안장점(Saddle Point)은 .. 이도 저도 아닌 경우이다. 즉 극소점도 아니고 극대점도 아닌 경우.  ex) y = x^3에서 x=0일때.

- ex) y = x^3에서 x=0일때. 이 경우 기울기가 음수에서 양수로 바뀌거나 양수에서 음수로 바뀌는 건 아니다. 기울기는 계속 양수인데, 기울기가 평평해졌다가 다시 가팔라지는 경우.. 이런 경우가 안장점이다.

11. 고차원에서의 미분

- ex) f(x,y) = x^2+2y^2일 경우. 머릿속으로 생각해야 된다 .. 그리기 어려움. 이럴 땐 편미분을 해야 한다. (함수를 변수 하나에 대해서만 미분하는 것!)

- f(x,y) * d/dx에선(x에 대한 편미분) x를 제외한 나머지 변수를 상수취급한다.(즉 미분할 땐 날려버린다) -> 2x

- f(x,y) * d/dy에선(y에 대한 편미분) y를 제외한 나머지 변수를 상수취급한다.(즉 미분할 땐 날려버린다) -> 4y

- f(x,y) * d/dx + f(x,y) * d/dy => 벡터f(x,y) = [2x, 4y] 벡터f(1,1) = [2, 4]

- y = 1이 고정돼 있고, x = 1일 때 함수 f의 기울기는 2! (x에 대한 경우)

- x = 1이 고정돼 있고, y = 1일 때 함수 f의 기울기는 4! (y에 대한 경우)


# <2> 기본 지도 학습 알고리즘들

## (1) 선형 회귀 (Linear Regression)

### 1. 선형 회귀

- 전체 데이터들을 가장 잘 반영하는 '선 1개'(최적선)를 찾는 것이다. (기울기)

### 2. 선형 회귀 용어

- 머신 러닝은 지도 학습, 비지도 학습, 강화 학습으로 이뤄져 있다고 했다. 

- '집 값'이라는 답이 있으므로, 선형 회귀는지도 학습 알고리즘이라고 할 수 있다.

- 분류 : 집을 아파트인지 오피스텔인지 주택인지 알아내고자 하는 것 <-> 회귀 : 집값을 예측하는 것

- 목표 변수(target varible) : 맞추려고 하는 값<br>입력 변수(input variable) : 맞추는데 사용하는 값

ex) 집 크기를 통해 집 값을 예측한다고 할 때, 집 크기가 입력 변수이고 집 값이 목표 변수이다.

- 학습 데이터 : 우리가 프로그램을 학습시키기 위해 사용하는 데이터. m = 50이라면 50개의 데이터로 학습 시킨다는 뜻. 입력변수는 x, 목표변수는 y로 칭한다. 1번 데이터의 입력변수는 x의 1제곱을 쓸때와 같은 표기법으로 쓰고, 1번 데이터의 목표변수는 y의 1제곱을 쓸때와 같은 표기법으로 쓴다.


### 4. 가설 함수

- 데이터들에 가장 잘 맞는 최적선을 찾는 게 선형 회귀인데, 이 선을 나타내는 함수(직선이므로 1차함수임)를 가설 함수(hypothesis function)라고 한다. y = ax+b이므로 결국 a와 b를 찾아내는 과정인 것..

- 가설 함수는 y = ax + b 대신 h(x) = θ0 + θ1x 라고 쓴다.

- 변수가 많아지면 f = ax+by+cz+dq+er+... 이렇게 되게 복잡해지는데, 가설 함수를 이용해서 함수를 나타내면 h(x) = θ0 + θ1x1 + θ2x2 + ... 이런식으로 좀더 쉽게 표현할 수 있다.

- 즉 선형회귀는 가장 적절한 θ값을 찾아내는 것..

### 6. 평균 제곱 오차 (MSE)

- 가설 함수 평가법임. (Mean Sqaured Error)

- 이 데이터와 가설 함수가 평균적으로 얼마나 떨어져 있는지를 나타내는 방식. <b>오차를 제곱한 값들을 더하고 총 데이터 개수만큼 나눈다.<b> 

- 제곱을 하면 음수도 양수로 바뀌므로 모든 오차를 양수로 통일할 수 있다. '제곱'을 더하는 이유는, 더 큰 오차를 부각하기 위해서이다. 2 -> 4, 10 -> 100을 보면 알겠지만, 기존 값의 차이는 8이나 제곱한 값의 차이는 96이다. 오차가 큰 것에 대해 더 큰 페널티를 주기 위함이라고 생각하면 된다.

- MSE가 가장 낮은 가설함수가 가장 오차가 적은(가장 해당 데이터셋에 잘 맞는) 것이라는 결론에 이를 수 있다.

### 7. 평균 제곱 오차 일반화

- 1/m * 시그마(1에서m까지) (hθ(xi) - yi)제곱 이다.

- 즉, (i번째 데이터 예측값 - i번째 아웃풋(실제값)) = (i번째 데이터 예측 오차)를 제곱한 것을 i를 1부터 m까지 다 더한 후에 그것을 m으로 나눠서 평균을 내는 것이다.

- 더 쉽게 설명하면, 오차값의 제곱을 m번 더한 후에 그것을 m으로 나누는 것이다.

- 평균 제곱 오차가 크다는 건, 가설 함수가 데이터에 잘 안 맞다는 것이고, 작다는 건 잘 맞다는 것이다.

### 9. 손실 함수(Loss Function)

- 가설 함수의 성능을 평가하는 함수

- 손실 함수가 작으면 : 가설 함수가 데이터에 잘 맞다는 것이고, 크다는 건 잘 안 맞다는 것이다. 손실 함수는 J(θ) = 1/2m * 시그마(1에서m까지) (hθ(xi) - yi)제곱 이다. 가설 함수와 다른 점은, input이 θ라는 점이고, 분모가 1/2m 이라는 것이다.

- hθ(x) = θ0 + θ1x인데, θ는 우리가 바꿀 수 있는 값이다. 결국 손실 함수의 output은 θ 값을 어떻게 설정하는가에 따라 달려있다.

### 11. 경사 하강법 개념 (Gradient Descent)

- hθ(x) = θ0 + θ1x에서, θ0과 θ1을 조금씩 조율해서 최적선을 찾는다.

- 손실 함수의 output을 최소화해야 좋은 데이터가 된다. 2차함수로 지정된 손실 함수의 극소점을 찾아야 함.

### 12. 경사 하강법 테크닉

- 손실 함수의 극소점으로 가는 게 목표. 가설 함수를 계속 개선해서 아래로 내려가면 됨.

- (θ0, θ1) = (-3, -3), 벡터J(θ) = [2θ0, 4θ1] 라고 가정했을 때 현재 기울기는 [-6, -12] 이다. 2θ0은 J를 θ0로 편미분한 값이고, 4θ1은 J를 θ1로 편미분한 값이다. 새로운 θ0 = -3-a * (-6) 이렇게 해준다. 여기서 a는 학습률이라고 하는데, 학습률은 경사를 타고 갈 때 얼마나 많이 움직일지 그 정도를 나타내는 것이다. 클수록 많이 움직이는 것. a를 0.1이라고 가정해보자. 그러면 새로운 θ0은 -3-0.1 * (-6) = -2.4가 된다. 새로운 θ1은 -3-0.1 * (-12) = -1.8이 된다. [-3, -3] 에서 [-2.4, -1.8]이 됨.

- 새로운 θ0 = θ0 - (학습률) * J(θ0, θ1)을 θ0으로 편미분 한 값<br>새로운 θ1 = θ1 - (학습률) * J(θ0, θ1)을 θ1로 편미분 한 값

- 주의해야 될 점은, 새로운 θ0을 만들고 나서 새로운 θ1을 만드는 과정을 진행할 때 그 '새로운 θ0'이 들어가면 안되고 '기존의 θ0'이 들어가야 된다는 점이다.

### 13. 경사 하강법 계산

- 새로운 θ0 = θ0 - (학습률) * J(θ0, θ1)을 θ0으로 편미분 한 값 에서,<br>새로운 θ0은, θ0 - (학습률) * (1/m) * (시그마 i가 1에서 m까지)(hθ(xi)-yi) 이다.

- 새로운 θ1 = θ1 - (학습률) * J(θ0, θ1)을 θ1로 편미분 한 값 에서,<br>새로운 θ1은, θ1 - (학습률) * (1/m) * (시그마 i가 1에서 m까지)(hθ(xi)-yi)*xi 이다.

- 이제 θ0과 θ1을 업데이트하는 공식이 모두 준비되었으므로, 이걸 이용해서 θ0과 θ1을 반복적으로 업데이트하면 손실을 최소화하는 최적선을 구해낼 수 있다!

### 14. 선형 회귀 구현하기 쉽게 표현하기

- hθ(x)와 벡터로써의 y의 차이를 계산하면, hθ(x) - y = [hθ(x1제곱)-(y1제곱), hθ(x2제곱)-(y2제곱), ...] 의 형태. (물론 실제 제곱은 아니고 .. 그런 형태라는 것) 이것을 편의상 error로 부르기로 한다.

- 벡터 error의 모든 원소의 평균을 μerror 라고 표현할 수 있다. 그럼 θ0과 θ1을 업데이트하는 공식을 다르게는, θ0 <- θ0 - (학습률) * μerror, θ1 <- θ1 - (학습률) * μ(error * x) 로 표현할 수 있게 된다.

- 지금은 저걸 굳이 왜 바꾸지 라고 생각할 수 있지만, θ0 <- θ0 - (학습률) * μerror, θ1 <- θ1 - (학습률) * μ(error * x) 로 바꾸는 게 기존 식 그대로 나타내는 것보다 훨씬 간편하다는 것을 나중에 알게 될 것!

### 15. 선형 회귀 가설 함수 구현하기

- prediction 함수 : 주어진 가설 함수로 얻은 결과를 리턴하는 함수이다. 파라미터로는 θ0을 나타내는 숫자형 변수 theta_0, θ1을 나타내는 숫자형 변수 theta_1, 그리고 모든 입력 변수 벡터 x들을 나타내는 numpy 배열 x를 받는다.

### 19. 학습률 알파

- 학습률 a를 그냥 작은 숫자라고만 하고, '경사를 내려갈 때마다 얼마나 많이 그 방향으로 갈 건지를 결정하는 변수'라고 했는데, 사실 학습률은 그렇게 간단한 게 아니라, 적당한 값으로 항상 유지되어야 한다. 너무 작아도 커도 안된다.

- 1) 학습률 a가 너무 큰 경우 : a가 크면 클수록 경사 하강을 한 번 할 때마다 θ의 값이 많이 바뀐다. 최고차항이 양수인 2차함수 그래프 J(θ)에서, 양 쪽 변을 성큼성큼 왔다갔다 하면서 진행이 될 것. a가 너무 크면 경사 하강법을 진행할 수록 J의 최소점에서 멀어질 수도 있다.

- 2) 학습률 a가 너무 작은 경우 : 경사 하강을 한 번 할 때마다 θ가 계속 매우 조금씩만 움직이게 된다. 너무나도 작게 되면 최소 지점을 찾는 게 너무 오래 걸린다. 극소점까지 갈 수는 있으나 시간이 너무 오래 걸리기 때문에 효율성 면에서 매우 떨어진다.

- 3) 그래서 a를 적당한 크기로 하는 게 중요하다. 최적의 학습률일 때 손실은 횟수가 늘어날 수록 줄어들게 되는데, a가 너무 크다면 횟수가 늘어날 수록 손실이 커지게 되고 반대로 너무 작다면, 최적의 학습률일때의 그래프와 모양은 동일하지만 횟수가 매우 매우 커지게 된다!

- 4) 일반적으로 1.0~0.0 사이의 숫자로 정하고 (1, 0.1, 0.01, 0.001 또는 0.5, 0.05, 0.005 이런 식으로), 여러 개를 실험해보면서 경사 하강을 제일 적게 하면서 손실이 잘 줄어드는 학습률을 선택한다.

### 20. 모델 평가하기

- 이 모델이 얼마나 결과를 정확히 예측하는지를 평가해야 한다.

- 평균 제곱근 오차 (root mean square error(RMSE)) -> 평균 제곱 오차의 루트값. 단위가 원 일 때, 평균 제곱 오차를 하면 원의 제곱 형태가 되는데, 이는 쉽사리 다가오지 않는다. 그래서 루트를 씌워 원 단위로 돌아가게 해주는 것.

- 학습시킨 모델(최적선)과 데이터를 비교해서 평균 제곱근 오차를 구하면 예측값을 구할 수 있지만, 이는 사실 이미 '학습'된 모델이기 때문에 평균 제곱근 오차가 낮게 나오는 건 당연한 일이다. 그래서, 모델을 학습시키기 위한 데이터와 평가하기 위한 데이터를 나눈다.

- 학습 데이터 : training set, 평가 데이터 : test set 이렇게 큰 데이터셋 내에 있는 데이터들을 학습과 평가 데이터로 따로 나눔으로써 우리의 모델을 좀 더 신빙성 있게 평가할 수 있다.


## (2) 다중 선형 회귀 (Multiple Linear Regression)

### 1. 다중 선형 회귀

- 문제를 최대한 간단하게 하기 위해서 입력 변수 1개로 목표 변수를 예측하는 것을 배웠었는데, 입력 변수 1개만으로 목표 변수를 예측 하는 경우는 매우 드물다. 그래서 보통 많은 입력 변수를 통해 목표 변수를 예측한다. 이것을 다중 선형 회귀라고 한다.

- 다중 선회 회귀는 시각적으로 표현하기가 힘들다. 하지만 시각적으로 표현하기 어렵다 뿐이지 지난 챕터에서 배운 선형 회귀와 거의 똑같다고 보면 된다.

### 2. 다중 선형 회귀 표현법

- 선형 회귀에서, 입력변수는 x로, 목표변수는 y로 나타냈었다.

- 입력변수가 많은 경우, x1, x2, x3 등으로 나타낸다. 입력 변수는 많지만 목표 변수는 보통 하나이다. 각 모델의 경우 x의 (1제곱)과 y의 (1제곱), x의 (2제곱)과 y의 (2제곱)의 형태로(문자의 우측 상단에 숫자를 괄호 안에 작게 나타내라는 뜻).

- i번째 데이터의 j번째 속성 : xi의 (j제곱) 형태.

### 3. 다중 선형 회귀 가설 함수

- 가설함수는, 입력변수가 1개였을 때는 hθ(x) = θ0 + θ1x였다.

- 다중 선형 회귀에서는, hθ(x) = θ0 + θ1x1 + θ2x2 + ... + θnxn의 형태이다. 이전처럼 θ0은 상수항이다. 뭔가 복잡해진 것 같지만, 이 함수도 1차함수이다. 단지 좀 길어진 것일 뿐. x1 = 집 크기, x2 = 방 수, x3 = 지하철 거리, x4 = 건물의 나이 이런 식이고, θ1 = 집크기가 미치는 영향력, ~ 이렇게 나타내는 것이다.

- 다중 선형 회귀에서도 목적은 θ값을 조금씩 조율하면서 학습 데이터에 가장 잘 맞는 θ값들을 찾아내는 것이다. 

- θ = [θ0, θ1, θ2, ... θn], x = [1, x1, x2, ... xn]에서, θTx = θ0 + θ1x1 + θ2x2 + ... + θnxn = h0(x)와 동일해진다. 가설 함수를 이렇게 풀어서 표현할 수도 있지만, 이렇게 벡터로 간결하게 표현할 수 있다는 점을 기억해 두자.

### 6. 다중 선형 회귀 경사 하강법

- 데이터들에 대해 선형 회귀로 하려는 건, 최적선을 구하는 것이다. 이 최적선을 찾기 위해 시도하는 것을 가설 함수라고 한다. 가설 함수가 얼마나 좋은지, 어떻게 개선해야 하는지를 알기 위해서는 평가 기준이 필요한데, 손실 함수가 그 기능을 한다. 가설 함수가 어떤 세타값을 쓰느냐에 따라 결괏값이 달라진다.

- 다중 선형 회귀에서는, 선형 회귀와 똑같다. J(θ) = 1/2m * 시그마(1에서m까지) (hθ(xi) - yi)제곱 이다.

- 입력 변수가 1개일 때는, θ0과 θ1만 업데이트 하면 되는데, 새로운 θ0 = θ0 - (학습률) * J(θ0, θ1)을 θ0으로 편미분 한 값<br>새로운 θ1 = θ1 - (학습률) * J(θ0, θ1)을 θ1로 편미분 한 값 이다. <br>여기서 θ0과 θ1뿐만 아니라 θn까지 이어진다는 것의 차이일 뿐..

### 8. 다중 선형 회귀 구현하기 쉽게 표현하기

- X =<br>[x0(1제곱) x1(1제곱) ... xn(1제곱)]<br>[x0(2제곱) x2(2제곱) ... xn(2제곱)]<br>...<br>[x0(m제곱) x2(m제곱) ... xn(m제곱)] 의 형태로 나타낼 수 있다.<br>기본적으로 한 열에 하나의 데이터를 표현하는 건 똑같다. (제곱) 으로 나타낸 부분은 몇 번째 데이터 인지를, 그리고 x 옆에 달려있는 숫자는 몇 번째 속성인지를 나타낸다.

- 여기서도 error = Xθ-y = [hθ(x(1제곱)-y(1제곱)) hθ(x(2제곱)-y(2제곱)) ... hθ(x(m제곱)-y(m제곱))] 의 형태로 나타난다.

- 경사 하강법 같은 경우 새로운 θ은, θ - (학습률) * (1/m) * (XT * (Xθ-y)) 와 같이 간단한 행렬식으로 표현할 수 있다. 행렬식으로 표현할 수 있기만 하면 numpy를 이용해서 수학식들을 쉽게 구현할 수 있다.

### 11. 정규 방정식

- 지금까지는 손실을 줄이기 위해 경사 하강법을 사용했었는데, 다른 방법도 있다.

- j(θ) = θ제곱 - 2θ + 1을 미분했을 때, j(θ)미분 = 2θ -2이다. 이게 0이 되는 θ값을 구함으로써 구할 수도 있는데, 이것을 정규 방정식이라고 한다. 이걸 쉽게 표현하면 ..

- θ = (XtX)역행렬 * Xt*y 에다가 대입만 하면 됨. 이 식을 계산하면, 손실 함수의 기울기가 0이 되는 θ를 구할 수 있다.

### 14. 경사 하강법 vs 정규 방정식

- 두 가지의 각자 장단이 뚜렷한데, 둘 중에 어떤 걸 선택해야 하는 지에 대해 절대적으로 정해진 건 없다. 입력 변수(속성)의 수가 엄청 많을 때는(1000개를 보통 기준으로 함) 경사 하강법을, 비교적 입력 변수의 수가 적을 때는 정규 방정식을 사용한다.

### 15. Convex 함수

- 극소값이나 극대값이 많은 경우.. 경사 하강법을 쓰든 정규 방정식을 쓰든 경사가 0인 부분에서 종료가 되고, 방정식을 해결해서 구한 많은 지점들 중에 어떤 지점이 최소점인지를 알 수가 없으므로 두 방법 다 무용지물이다.

- 손실 함수가 최고차항의 계수가 양수인 이차함수의 형태로 생긴 경우, 그러니까 아래로 볼록(convex)한 경우 어떤 지점에서 경사 하강을 시작하더라도 항상 손실 함수의 최소 지점을 찾을 수 있고, 정규 방정식을 이용해서 최소점을 구할 수 있다!

- convex 함수에서는 항상 경사 하강법이나 정규 방정식을 이용해서 최소점을 구할 수 있는 반면, non-convex 함수애서는 구한 극소점이 최소점이라고 확신할 수 없다.

#### 선형 회귀의 평균 제곱 오차

- 선형 회귀에서는, 가정 함수의 예측값들과 실제 목표 변수들의 평균 제곱 오차(MSE)를 손실 함수로 사용했는데, 다행히 선형 회귀 손실 함수로 사용하는 MSE는 항상 convex 함수이다. 그러니까, 선형 회귀를 할 때에는 경사 하강법을 하거나 정규 방정식을 하거나 항상 최적의 θ 값들을 구할 수 있는 것!

